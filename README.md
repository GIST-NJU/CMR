# Composite Metamorphic Relations (CMRs) for DNN Testing

This repository is supplementary to the following paper **"How Composite Metamorphic Relations Enhance Test Effectiveness of DNN Testing: An Empirical Study"**.

### ğŸ’» Project Structure

```
CMR
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ source            # Source test images
â”‚Â Â  â”œâ”€â”€ followup          # Follow-up test images
â”‚Â Â  â””â”€â”€ human_validation  # Sampled follow-up test images for human validation
â”œâ”€â”€ models                # DNNs under test
â”œâ”€â”€ figures               # Figures for RQs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ results
â”‚Â Â  â”œâ”€â”€ predictions       # Original prediction results
â”‚Â Â  â”œâ”€â”€ validity          # Automated validator and validation results
â”‚Â Â  â”œâ”€â”€ errors            # Failure rate and fault type metrics
â”‚Â Â  â”œâ”€â”€ features          # Feature representations of test images
â”‚Â Â  â””â”€â”€ samples           # Indices of Sampled CMRs and source test images for evaluating augmented models
â”œâ”€â”€ RQs.ipynb             # Codes for analysing data and generating figures and tables
â””â”€â”€ src                   # Codes for reproducing experiment
```

### âš™ï¸ Requirements 

Run the following command to install the dependencies required:

```bash
conda create -n cmr python=3.8.18
conda activate cmr
pip install -r requirements.txt
```

### ğŸ“¦ Subject Datasets and DNNs

The experiment is performed on five DNNs, trained on five popular image recognition related datasets:

* **AlexNet @ MNIST** (single-label classification)
* **DenseNet @ Caltech256** (single-label classification)
* **MSRN @ VOC** (multi-label classification)
* **MLD @ COCO** (multi-label classification)
* **Faceptor @ UTKFace** (regression-based facial image estimation)

The DNNs trained can be download from [zenodo](https://doi.org/10.5281/zenodo.18303100) in `models/`. You need to put these models in the `models/` directory.

The test sets of these datasets (all of which are used as source test images) can be downloaded from [zenodo](https://doi.org/10.5281/zenodo.18303100) in `data/source/` or automatically downloaded when running the code, **except for COCO**.
You need to put these images downloaded from zenodo in the `data/source/` directory.

To prepare source test images for **COCO**, you should download the following archives and extract them as the following directory structure:
```
data/source/COCO/
â”œâ”€â”€ test2014                  # from http://images.cocodataset.org/zips/test2014.zip
â”œâ”€â”€ train2014                 # from http://images.cocodataset.org/zips/train2014.zip
â”œâ”€â”€ val2014                   # from http://images.cocodataset.org/zips/val2014.zip
â”œâ”€â”€ image_info_test2014.json  # from http://images.cocodataset.org/annotations/image_info_test2014.zip
â”œâ”€â”€ instances_train2014.json  # from http://images.cocodataset.org/annotations/annotations_trainval2014.zip
â””â”€â”€ instances_val2014.json    # same as above
```


### ğŸ›ï¸ Component MRs

The experiment involves a set of seven representative component MRs. The implementation of these MRs is in the `src/mr_utils.py` file.

In our implementation, each MR is assigned a unique identifier (i.e., its index) in the code.
Throughout the experiments, MRs are referenced exclusively by these identifiers. When CMRs are applied to generate follow-up test images, the corresponding sequence of MR identifiers is denoted either as an ordered tuple or as a concatenated identifier string.
The mapping between MRs (in paper) and their identifiers is as follows:

| MR   | Image Transformation | Identifier Used |
| ---- | -------------------- | --------------- |
| MR1  | Brightness           | 1               |
| MR2  | Contrast             | 3               |
| MR3  | Sharpness            | 2               |
| MR4  | Blur (Gaussian)      | 4               |
| MR5  | Rotation             | 0               |
| MR6  | Shear (Horizontal)   | 5               |
| MR7  | Translation          | 6               |

For example, the CMR `(3, 1)` (tuple) or the concatenated form `31` (str) indicates that the follow-up test images are generated by first applying `Contrast` (identifier 3) and then `Brightness` (identifier 1).


### ğŸ“ƒ Experimental Data

The `data/followup/` directory contains the follow-up test images generated by applying CMRs on source test images. The follow-up test set is too large (approximately 78 TB) to provide here. Nevertheless, some randomly selected follow-up images for manual verification (in RQ1) can be downloaded from [zenodo](https://doi.org/10.5281/zenodo.18303100) in `data/human_validation/`, which should be downloaded to `data/human_validation/`.

The `figures` directory contains all figures presented in the paper:

* *RQ1*: Proportion of valid follow-up test images generated by CMRs
* *RQ2*: The overlap of fault types that are uncovered by CMR, the strongest MR, and all component MRs
* *RQ3.1*: Difference in Failure Rate (DFR) and Fault Type (DFT) between CMR and its strongest and average-performing component MR
* *RQ3.2*: Relationship between DFR (DFT) and âˆ†M values observed

The `results` directory (can be downloaded from [zenodo](https://doi.org/10.5281/zenodo.18303100) in `results/`) contains the raw experimental results.

##### 1. Original Prediction Outputs

The `predictions` directory contains the original prediction outputs of the model on source and follow-up images, including:

* `[Dataset]/[Dataset]_[DNN]_source.npy` or `[Dataset]/[Dataset]_[DNN]_source.csv`: the prediction results on source test images. Each element in the NumPy array or each row in the CSV corresponds to a single source test image.
* `[Dataset]/[Dataset]_[DNN]_followup.npy`: the prediction results on follow-up test images generated by applying the specified CMR. The data is organized as a `dict`, where each key corresponds to a CMR and its value is a `list` or `dataframe` of prediction results for all follow-up test images generated by that CMR.


##### 2. Validity of Test Inputs

The `validity` directory contains the files for running the *SelfOracle* method, and the results obtained, including:

* `[Dataset]_VAE.pth`: the VAE trained for each dataset
* `[Dataset]_threshold.txt`: the threshold that is automatically derived based on the VAE and rate of false alarm (0.01%)
* `[Dataset]_validity.npy`: the validity of follow-up test images that *SelfOracle* determines, organized in a NumPy array. Each element corresponds to a CMR, and contains a list of float values predicted by the VAE for all follow-up test images generated by applying this CMR. A value less than or equal to the threshold indicates that the corresponding follow-up test image is semantically *valid*.

This directory also contains the follow-up test images sampled for manual analysis, as displayed in the `data/human_validation/` directory. The `human_validation.csv`  file gives the manual validation results: Each row corresponds to a follow-up test image that is manually determined as semantically *invalid*, where the three columns indicate the dataset name, the validity determined by *SelfOracle* (1 indicates valid), and the filename of the follow-up test image (e.g., `012_2780_2.png`). Here, the filename encodes that the follow-up image was generated from the source image with index `2780`, using CMR with identifier `012`, and the label of this image is `2`.

##### 3. Failure and Fault Revelation

The `errors` directory contains the image indices of failed test images and revealed fault types for each individual MR and CMR, as saved in `failures_[DNN].pkl` and `faults_[DNN].pkl` files, respectively. The data is organized in a dictionary format, where each key corresponds to a MR or CMR, and its value contains the relevant information of failed test images or revealed fault types.

To facilitate an intuitive inspection of the Failure Rate (FR) and Fault Type (FT) for each MR or CMR (based on preliminarily processed data that is more detailed than that reported in the paper), we additionally provide `failure_rates.csv` and `fault_types.csv` in this directory. Each row in these CSV files corresponds to a MR/CMR for each DNN, along with their calculated FR/FT values.


##### 4. Complementary MRs

The `features` directory contains the extracted features of source and follow-up test images for analysing complementary MRs, saved as `[Extractor]/[Dataset].pt` and `[Extractor]/[Dataset]_[MR].pt` files, respectively. Here, `[Extractor]` indicates the feature extractor used (`vgg16` for `Caltech256`, `VOC`, and `COCO`, `lenet50` for `MNIST`, and `insightface` for `UTKFace`), and `[MR]` is the identifier of the MR applied to generate the follow-up test images. These files are PyTorch tensors with shape `(N, D)`, where `N` is the number of test images and `D` is the dimension of the extracted feature representation.

Note that these files contain the raw extracted features. The features were then reduced to 8 dimensions using PCA (Principal Component Analysis), and the resulting vectors were used to compute âˆ†M in the `RQs.ipynb` notebook.

The extractors used for feature extraction are pre-trained models available directly from the corresponding packages, except for `lenet50`, whose pre-trained model is provided as `results/features/lenet50/lenet50_mnist.pth`.

##### 5. Sampled CMRs and Source Images for Evaluating Augmented Models

The `samples` directory contains the indices of sampled CMRs and source images used for evaluating the augmented models, saved as `[Dataset]_[DNN]_cmr[Size].pkl` and `[Dataset]_[Size].pkl`, respectively.
Where `[Size]` indicates the number of sampled CMRs or source images (e.g., `50` indicates that 50 CMRs are sampled).
These indices correspond to the original test images in the dataset and can be used to retrieve the actual images for prediction and evaluation.

### ğŸ› ï¸ Reproducing Experiment

The following scripts can be used to reproduce the complete experiment and evaluation process. To generate tables and figures presented in the paper, run the commands in `RQs.ipynb`.

##### 1. Generate follow-up test images

```bash
python src/generate_followup.py --dataset COCO --strength 2
```
The `--dataset` parameter specifies the dataset name, and `--strength` specifies the composition strength applied. The follow-up test images generated by employing the input mappings of the specified CMRs will be saved in `data/followup/[Dataset]/[CMR]` directory (e.g., `data/followup/COCO/31`).

##### 2. Validate follow-up test images

```bash
python src/selforacle.py --dataset COCO
```
This will run the *SelfOracle* method to determine the validity of test images in `data/followup/[Dataset]/[CMR]` directory, and produce results as included in  `results/validity/` directory.

##### 3. Execute DNNs Under Test

```bash
python src/predict.py --dataset COCO            # execute source test images
python src/predict.py --dataset COCO --followup # execute follow-up test images
```
This will run the DNN to make predications of the test images, and produce results as included in  `results/predictions/` directory.

##### 4. Evaluate Failure and Fault Revelation Capability

```bash
python src/count_failure_fault.py --dataset COCO
```
This will base on the prediction outputs to calculate the failures and faults, and produce the files as included in `results/errors/` directory.

##### 5. Calculate the âˆ†M Measure 

```bash
python src/extract_features.py
```

This will calculate the feature representation of each test image, and produce the results as included in `results/features/` directory.

##### 6. Data Augmentation

```bash
# train with data augmentation
python src/data_augment.py --dataset COCO --augment online
# sample source images and CMRs for evaluating augmented models
python src/data_augment/select_samples.py --cmr_num 50 --source_num 1000
# predict using augmented models
python src/predict.py --dataset COCO --augment online --cmr_num 50 --source_num 1000
# evaluate failure and fault revelation capability
python src/count_failure_fault.py --dataset COCO --cmr_num 50 --source_num 1000
python src/count_failure_fault.py --dataset COCO --augment online --cmr_num 50 --source_num 1000
```

The `--augment` parameter specifies the data augmentation method used. Here, `online` indicates the online (i.e., on-the-fly) data augmentation method based on CMRs. The `--cmr_num` and `--source_num` parameters specify the number of CMRs and source test images sampled for evaluating the augmented models, respectively. If `--source_num` set to `0`, all source test images will be used.

This will train the DNN using augmented dataset and save the checkpoints (with `Aug_online` suffix following the names of DNN) as included in `models` directory. Then it will record indices of sampled CMRs and source images as included in `results/samples/` directory, followed by producing predication results and the failures and faults as included in `results/predictions/` and `results/errors/`, respectively.
