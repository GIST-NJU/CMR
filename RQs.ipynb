{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import permutations, combinations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src.mr_utils import mrs\n",
    "\n",
    "mr_num = len(mrs)\n",
    "\n",
    "strength_max = 5\n",
    "datasets = ['MNIST', 'Caltech256', 'VOC', 'COCO', 'UTKFace']\n",
    "test_cases_num = {'MNIST': 10000, 'Caltech256': 3061, 'VOC': 4952, 'COCO': 40775, 'UTKFace': 3287}\n",
    "models = {'MNIST': ['AlexNet'],\n",
    "\t\t'Caltech256': ['DenseNet121'],\n",
    "\t\t'VOC': ['MSRN'],\n",
    "\t\t'COCO': ['MLD'],\n",
    "  \t\t'UTKFace': ['Faceptor']}\n",
    "model_names = ['MNIST_AlexNet_9938', 'Caltech256_DenseNet121_6838', 'VOC_MSRN', 'COCO_MLD', 'UTKFace_Faceptor']\n",
    "augmented_model_names = [\n",
    "\t'MNIST_AlexNet_Aug_online_9938', 'Caltech256_DenseNet121_Aug_online_7187', 'VOC_MSRN_Aug_online', 'COCO_MLD_Aug_online', 'UTKFace_Faceptor_Aug_online'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(dataset, model, augment=None):\n",
    "\tif not augment:\n",
    "\t\tfor i in range(len(model_names)):\n",
    "\t\t\tif f\"{dataset}_{model}\" in model_names[i]:\n",
    "\t\t\t\treturn model_names[i]\n",
    "\telse:\n",
    "\t\tfor i in range(len(augmented_model_names)):\n",
    "\t\t\tif f\"{dataset}_{model}_Aug_{augment}\" in augmented_model_names[i]:\n",
    "\t\t\t\treturn augmented_model_names[i]\n",
    "\n",
    "def get_validity(dataset):\n",
    "\tfilename_validity = 'results/validity/' + dataset + '_validity.npy'\n",
    "\tfilename_threshold = 'results/validity/' + dataset + '_threshold.txt'\n",
    "\tvalidity = np.load(filename_validity, allow_pickle=True).item()\n",
    "\twith open(filename_threshold) as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\tthreshold = float(lines[1].split(':')[1].strip())\n",
    "\t# print(threshold)\n",
    "\tfor mr in validity:\n",
    "\t\tfor i in range(len(validity[mr])):\n",
    "\t\t\tif validity[mr][i] <= threshold:\n",
    "\t\t\t\tvalidity[mr][i] = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tvalidity[mr][i] = False\n",
    "\treturn validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_all, faults_all, total_fault_num_all = {}, {}, {}\n",
    "validity_all = {}\n",
    "for model_name in model_names:\n",
    "\ttry:\n",
    "\t\tprint(model_name, end=' ')\n",
    "\t\twith open(f'results/errors/failure_{model_name}.pkl', 'rb') as f:\n",
    "\t\t\tfailure = pickle.load(f)\n",
    "\t\tfailures_all[model_name] = failure\n",
    "\t\twith open(f'results/errors/fault_{model_name}.pkl', 'rb') as f:\n",
    "\t\t\tfaults = pickle.load(f)\n",
    "\t\tfaults_all[model_name] = faults\n",
    "\t\ttotal_fault_num_all[model_name] = len(set.union(*[set(f) for f in faults.values()]))\n",
    "\t\tif model_name.split('_')[0] not in validity_all:\n",
    "\t\t\tvalidity = get_validity(model_name.split('_')[0])\n",
    "\t\t\tvalidity_all[model_name.split('_')[0]] = validity\n",
    "\t\tprint(f\"done, {len(failure)=}, {len(faults)=}, {total_fault_num_all[model_name]=}.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1 Validity\n",
    "\n",
    "How does the validity of follow-up test inputs generated by CMRs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(datasets):\n",
    "\tresult_selfOracle = validity_all[dataset]\n",
    "\n",
    "\tdata = [[] for _ in range(strength_max)]\n",
    "\tfor cmr in result_selfOracle.keys():\n",
    "\t\tif len(cmr) > strength_max:\n",
    "\t\t\tcontinue\n",
    "\t\tdata[len(cmr)-1].append(len(np.where(np.array(result_selfOracle[cmr]) == True)[0]) / \n",
    "\t\t\t\t\t\t\t\t\tlen(result_selfOracle[(0,)]) * 100)\n",
    "\n",
    "\tplt.figure(figsize=(3, 3))\n",
    "\tplt.boxplot(data, patch_artist=False)\n",
    "\n",
    "\tplt.xlabel(f'Composition strength ($k$)')\n",
    "\tplt.ylabel('Proportion of valid images')\n",
    "\n",
    "\tif dataset != 'MNIST':\n",
    "\t\tplt.ylim(96, 100.1)\n",
    "\t\tplt.yticks([96, 97, 98, 99, 100])\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'figures/RQ1/{dataset}_validity.pdf', dpi=600)\n",
    "\t# plt.show()\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "human_validation_df = pd.read_csv(\"results/validity/human_validation.csv\")\n",
    "selected_check_num = {\n",
    "    \"valid\": {},\n",
    "    \"invalid\": {}\n",
    "}\n",
    "human_selected_invalid = {\n",
    "    \"valid\": {},\n",
    "    \"invalid\": {}\n",
    "}\n",
    "for dataset in datasets:\n",
    "    for val in ['valid', 'invalid']:\n",
    "        for strength in range(1, strength_max + 1):\n",
    "            dir_path = os.path.join(\"data\", \"human_validation\", dataset, str(strength), val)\n",
    "            selected_check_num[val][dataset] = selected_check_num[val].get(dataset, 0) + len(os.listdir(dir_path))\n",
    "        human_selected_invalid[val][dataset] = len(human_validation_df[\n",
    "            (human_validation_df['dataset'] == dataset) &\n",
    "            (human_validation_df['self_oracle_label'] == (1 if val == 'valid' else 0))\n",
    "        ])\n",
    "\n",
    "print(selected_check_num)\n",
    "print(human_selected_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tf_pn(dataset):\n",
    "    true_positive = selected_check_num[\"valid\"][dataset] - human_selected_invalid[\"valid\"][dataset]\n",
    "    true_negative = human_selected_invalid[\"invalid\"][dataset]\n",
    "    false_positive = human_selected_invalid[\"valid\"][dataset]\n",
    "    false_negative = selected_check_num[\"invalid\"][dataset] - human_selected_invalid[\"invalid\"][dataset]\n",
    "    return true_positive, true_negative, false_positive, false_negative\n",
    "print(\"Dataset & # Valid & # Invalid & # False Positive & # False Negative \\\\\\\\\")\n",
    "for dataset in datasets:\n",
    "    tp, tn, fp, fn = get_tf_pn(dataset)\n",
    "    print(f\"{dataset} & {selected_check_num['valid'][dataset]} & {selected_check_num['invalid'][dataset]} & {fp} & {fn} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Overall Test Effectiveness\n",
    "\n",
    "How effective are CMRs in DNN testing compared to their respective component MRs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the total failure ratio and the total number of faults\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\tprint(dataset, model)\n",
    "\t\tmodel_name = get_model_name(dataset, model)\n",
    "\t\tselected_source_num = test_cases_num[dataset]\n",
    "\t\tselected_cmr_num = 0\n",
    "\t\tfailure_CMR_MAX, failure_CMR_MEAN, failure_CMR_ALL = 0, 0, 0\n",
    "\t\tfault_CMR_MAX, fault_CMR_MEAN, fault_CMR_ALL = 0, 0, 0\n",
    "\n",
    "\t\tfault_cmr, fault_max, fault_union = 0, 0, 0\n",
    "\t\tfault1, fault2, fault3, fault4, fault5 = 0, 0, 0, 0, 0\n",
    "\n",
    "\t\tfor i in range(2, strength_max+1):\n",
    "\t\t\tfor cmr in permutations(range(mr_num), i):\n",
    "\t\t\t\tselected_cmr_num += 1\n",
    "\t\t\t\tfailure_com, fault_com = [], []\n",
    "\t\t\t\tfailure_com_count = 0\n",
    "\t\t\t\tfailure_com_union = set()\n",
    "\t\t\t\tfault_com_union = set()\n",
    "\t\t\t\tfault_com_max = set()\n",
    "\t\t\t\tfor com in cmr:\n",
    "\t\t\t\t\tfailure_com_union.update(failures_all[model_name][(com,)])\n",
    "\t\t\t\t\tfailure_com.append(len(failures_all[model_name][(com,)]))\n",
    "\t\t\t\t\tfailure_com_count += len(failures_all[model_name][(com,)])\n",
    "\t\t\t\t\tfault_com.append(len(set(faults_all[model_name][(com,)].values())))\n",
    "\t\t\t\t\tfault_com_union.update(set(faults_all[model_name][(com,)].values()))\n",
    "\t\t\t\t\tif len(set(faults_all[model_name][(com,)].values())) > len(fault_com_max):\n",
    "\t\t\t\t\t\tfault_com_max = set(faults_all[model_name][(com,)].values())\n",
    "\n",
    "\t\t\t\tfailure_CMR_MAX += (len(failures_all[model_name][cmr]) - max(failure_com)) / selected_source_num\n",
    "\t\t\t\tfailure_CMR_MEAN += (len(failures_all[model_name][cmr]) - sum(failure_com) / i) / selected_source_num\n",
    "\t\t\t\tfailure_CMR_ALL += (len(failures_all[model_name][cmr]) - failure_com_count / i) / selected_source_num\n",
    "\n",
    "\t\t\t\tfault_CMR_MAX += (len(set(faults_all[model_name][cmr].values())) - max(fault_com)) / total_fault_num_all[model_name]\n",
    "\t\t\t\tfault_CMR_MEAN += (len(set(faults_all[model_name][cmr].values())) - sum(fault_com) / i) / total_fault_num_all[model_name]\n",
    "\t\t\t\tfault_CMR_ALL += (len(set(faults_all[model_name][cmr].values())) - len(fault_com_union) / i) / total_fault_num_all[model_name]\n",
    "\n",
    "\t\t\t\tfault_cmr += len(set(faults_all[model_name][cmr].values()))\n",
    "\t\t\t\tfault_max += max(fault_com)\n",
    "\t\t\t\tfault_union += len(fault_com_union)\n",
    "\n",
    "\t\t\t\tfault1 += len(set(faults_all[model_name][cmr].values()) - fault_com_union)\n",
    "\t\t\t\tfault2 += len((set(faults_all[model_name][cmr].values()) & fault_com_union) - fault_com_max)\n",
    "\t\t\t\tfault3 += len(set(faults_all[model_name][cmr].values()) & fault_com_max)\n",
    "\t\t\t\tfault4 += len(fault_com_max - set(faults_all[model_name][cmr].values()))\n",
    "\t\t\t\tfault5 += len(fault_com_union - (set(faults_all[model_name][cmr].values()) | fault_com_max))\n",
    "\n",
    "\t\tdfr_max = failure_CMR_MAX / selected_cmr_num\n",
    "\t\tdfr_mean = failure_CMR_MEAN / selected_cmr_num\n",
    "\t\tdfr_all = failure_CMR_ALL / selected_cmr_num\n",
    "\t\tdft_max = fault_CMR_MAX / selected_cmr_num\n",
    "\t\tdft_mean = fault_CMR_MEAN / selected_cmr_num\n",
    "\t\tdft_all = fault_CMR_ALL / selected_cmr_num\n",
    "\t\tfault_cmr, fault_max, fault_union = fault_cmr/selected_cmr_num, fault_max/selected_cmr_num, fault_union/selected_cmr_num\n",
    "\t\tfault1, fault2, fault3, fault4, fault5 = fault1/selected_cmr_num, fault2/selected_cmr_num, fault3/selected_cmr_num, fault4/selected_cmr_num, fault5/selected_cmr_num\n",
    "\t\tprint(f\" & {dfr_max*100:.2f}\\% & {dfr_mean*100:.2f}\\% & {dfr_all*100:.2f}\\% & {dft_max*100:.2f}\\% & {dft_mean*100:.2f}\\% & {dft_all*100:.2f}\\% \\\\\\\\\")\n",
    "\t\tprint(f\"{total_fault_num_all[model_name]} {fault_cmr:.1f} {fault_max:.1f} {fault_union:.1f}\")\n",
    "\t\tprint(f\"{fault1:.0f} {fault2:.0f} {fault3:.0f} {fault4:.0f} {fault5:.0f}\")\n",
    "\n",
    "\tprint('-------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3 Influence of CMR Creation Methods\n",
    "\n",
    "How do the different ways in creating CMRs influence their test effectiveness in DNN testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ3.1\n",
    "\n",
    "How does the number of component MRs influence the test effectiveness of CMRs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw box plot of Failure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def compare_with_com_failure_4boxplot(dataset, model):\n",
    "\tmodel_name = get_model_name(dataset, model)\n",
    "\tresult_delta_max, result_delta_mean = [], []\n",
    "\tsource_num = test_cases_num[dataset]\n",
    "\tfor i in range(1, strength_max): \n",
    "\t\tdelta_max, delta_mean = [], []\n",
    "\t\tcmr_num = 0\n",
    "\t\tfor cmr in permutations(range(mr_num), i + 1):\n",
    "\t\t\tcmr_num += 1\n",
    "\t\t\tcmr_failure_ratio = len(failures_all[model_name][cmr]) / source_num\n",
    "\t\t\tcom_failure_ratio = []\n",
    "\t\t\tfor mr in cmr:\n",
    "\t\t\t\tmr = (mr,)\n",
    "\t\t\t\tcom_failure_ratio.append(len(failures_all[model_name][mr]) / source_num)\n",
    "\t\t\tdelta_max.append(cmr_failure_ratio - max(com_failure_ratio))\n",
    "\t\t\tdelta_mean.append(cmr_failure_ratio - sum(com_failure_ratio) / len(com_failure_ratio))\n",
    "\t\tresult_delta_max.append(delta_max)\n",
    "\t\tresult_delta_mean.append(delta_mean)\n",
    "\treturn result_delta_max, result_delta_mean\n",
    "\n",
    "def draw_boxplot_failure(dataset, model):\n",
    "\tdelta_max, delta_mean = compare_with_com_failure_4boxplot(dataset, model)\n",
    "\tx_labels = range(2, strength_max + 1)\n",
    "\tgroup_labels = [r\"$\\mathit{DFR}_{\\mathit{best}}$\", r\"$\\mathit{DFR}_{\\mathit{mean}}$\"]\n",
    "\n",
    "\trecords = []\n",
    "\tfor i, x in enumerate(x_labels):\n",
    "\t\tfor group, data in zip(group_labels, [delta_max[i], delta_mean[i]]):\n",
    "\t\t\tfor value in data:\n",
    "\t\t\t\trecords.append({'X': x, 'Group': group, 'Value': value})\n",
    "\tdf = pd.DataFrame(records)\n",
    "\n",
    "\tplt.figure(figsize=(4, 4))\n",
    "\tsns.boxplot(x='X', y='Value', hue='Group', data=df, palette='Set2', width=0.6, flierprops=dict(marker='o', markersize=3, linestyle='none') )\n",
    "\t#plt.title('Failure ' + dataset)\n",
    "\tplt.xlabel(f'$k$')\n",
    "\tplt.ylabel('Difference in failure ratio')\n",
    "\tplt.legend()\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/RQ3.1/failure_{dataset}_{model}_box_plot.pdf\", dpi=300, bbox_inches='tight')\n",
    "\t#plt.show()\n",
    "\tplt.close()\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\tdraw_boxplot_failure(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw box plot of Fault\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def compare_with_com_fault_4boxplot(dataset, model):\n",
    "\tmodel_name = get_model_name(dataset, model)\n",
    "\tresult_delta_union, result_delta_max, result_delta_mean = [], [], []\n",
    "\tresult_cmr_union_insert, result_cmr_union_diff = [], []\n",
    "\tfor i in range(1, strength_max):\n",
    "\t\tdelta_union, delta_max, delta_mean = [], [], []\n",
    "\t\tfault_cmr_union_insert, fault_cmr_minus_union, fault_union_minus_cmr = [], [], []\n",
    "\t\tfor cmr in permutations(range(mr_num), i + 1):\n",
    "\t\t\tf_cmr = set(faults_all[model_name][cmr].values())\n",
    "\t\t\tcom_fault = []\n",
    "\t\t\tcom_fault_union = set()\n",
    "\t\t\tfor mr in cmr:\n",
    "\t\t\t\tmr = (mr,)\n",
    "\t\t\t\tcom_fault.append(len(set(faults_all[model_name][mr].values())))\n",
    "\t\t\t\tcom_fault_union.update(faults_all[model_name][mr].values())\n",
    "\n",
    "\t\t\tdelta_union.append(len(f_cmr) / total_fault_num_all[model_name] - len(com_fault_union) / len(com_fault) / total_fault_num_all[model_name])\n",
    "\t\t\tdelta_max.append(len(f_cmr) / total_fault_num_all[model_name] - max(com_fault) / total_fault_num_all[model_name])\n",
    "\t\t\tdelta_mean.append(len(f_cmr) / total_fault_num_all[model_name] - sum(com_fault) / len(com_fault) / total_fault_num_all[model_name])\n",
    "\n",
    "\t\t\tfault_cmr_union_insert.append(len(f_cmr & com_fault_union) / total_fault_num_all[model_name])\n",
    "\t\t\tfault_cmr_minus_union.append(len(f_cmr - com_fault_union) / total_fault_num_all[model_name])\n",
    "\t\tresult_delta_union.append(delta_union)\n",
    "\t\tresult_delta_max.append(delta_max)\n",
    "\t\tresult_delta_mean.append(delta_mean)\n",
    "\t\tresult_cmr_union_insert.append(fault_cmr_union_insert)\n",
    "\t\tresult_cmr_union_diff.append(fault_cmr_minus_union)\n",
    "\t#return result_delta_union, result_delta_max, result_delta_mean, result_cmr_union_insert, result_cmr_union_diff\n",
    "\treturn result_delta_max, result_delta_mean\n",
    "\n",
    "def draw_boxplot_fault(dataset, model):\n",
    "\tdelta_max, delta_mean = compare_with_com_fault_4boxplot(dataset, model)\n",
    "\n",
    "\tx_labels = range(2, strength_max + 1)\n",
    "\tgroup_labels = [r\"$\\mathit{DFT}_{\\mathit{best}}$\", r\"$\\mathit{DFT}_{\\mathit{mean}}$\"]\n",
    "\n",
    "\trecords = []\n",
    "\tfor i, x in enumerate(x_labels):\n",
    "\t\tfor group, data in zip(group_labels, [delta_max[i], delta_mean[i]]):\n",
    "\t\t\tfor value in data:\n",
    "\t\t\t\trecords.append({'X': x, 'Group': group, 'Value': value})\n",
    "\n",
    "\tdf = pd.DataFrame(records)\n",
    "\n",
    "\tplt.figure(figsize=(4, 4))\n",
    "\tsns.boxplot(x='X', y='Value', hue='Group', data=df, palette='Set2', width=0.6, flierprops=dict(marker='o', markersize=3, linestyle='none') )\n",
    "\t#plt.title('Failure ' + dataset)\n",
    "\n",
    "\tplt.xlabel(f'$k$')\n",
    "\tplt.ylabel('Difference in size of fault types')\n",
    "\tplt.legend()\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/RQ3.1/fault_{dataset}_{model}_box_plot.pdf\", dpi=300, bbox_inches='tight')\n",
    "\t# plt.show()\n",
    "\tplt.close()\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\tdraw_boxplot_fault(dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ3.2 Component\n",
    "\n",
    "How do the different choices of component MRs influence the test effectiveness of CMRs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check delta Effectiveness w.r.t. Failure\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import spearmanr\n",
    "import re\n",
    "\n",
    "pca_n_components = 8\n",
    "\n",
    "for dataset in datasets:\n",
    "\tvalidity_followup = validity_all[dataset]\n",
    "\tif dataset == 'MNIST':\n",
    "\t\textractor = 'lenet5'\n",
    "\telif dataset == 'UTKFace':\n",
    "\t\textractor = 'insightface'\n",
    "\telse:\n",
    "\t\textractor = 'vgg16'\n",
    "\tsave_path = os.path.join('results', 'features', extractor, f'{dataset}.pt')\n",
    "\tsource__features = torch.load(save_path)\n",
    "\tfollowup__features_followup = []\n",
    "\tfor i in range(mr_num):\n",
    "\t\tsave_path = os.path.join('results', 'features', extractor, f'{dataset}_{i}.pt')\n",
    "\t\tfollowup__features_followup.append(torch.load(save_path))\n",
    "\tall_features = [source__features] + followup__features_followup\n",
    "\tall_features = [f.cpu().numpy() for f in all_features]\n",
    "\tall_features = np.concatenate(all_features, axis=0)\n",
    "\tpca = PCA(n_components=pca_n_components)\n",
    "\treduced_features = pca.fit_transform(all_features)\n",
    "\tsource_length = len(source__features)\n",
    "\tsource_features = reduced_features[:source_length]\n",
    "\tfollowup_features = reduced_features[source_length:]\n",
    "\tfollowup_separated = [followup_features[i * source_length:(i + 1) * source_length] for i in range(7)]\n",
    " \n",
    "\tfor model in models[dataset]:\n",
    "\t\tprint(dataset, model)\n",
    "\t\tmodel_name = get_model_name(dataset, model)\n",
    "\t\tfor i in range(2, strength_max+1):\n",
    "\t\t\tdeltadis, deltaeff = [], []\n",
    "\t\t\tfor com in combinations(range(mr_num), i):\n",
    "\t\t\t\tselect_com = com[0]\n",
    "\t\t\t\tfor j in range(1, len(com)):\n",
    "\t\t\t\t\tif len(failures_all[model_name][(com[j],)]) / sum(validity_followup[(com[j],)]) > len(failures_all[model_name][(select_com,)]) / sum(validity_followup[(select_com,)]):\n",
    "\t\t\t\t\t\tselect_com = com[j]\n",
    "\t\t\t\tdelta_d = []\n",
    "\t\t\t\tdelta_e = []\n",
    "\t\t\t\tfor cmr in permutations(com):\n",
    "\t\t\t\t\td, f, f_c, v_p_num = 0, 0, 0, 0\n",
    "\t\t\t\t\tfor index in range(source_length):\n",
    "\t\t\t\t\t\tif validity_followup[cmr][index] and validity_followup[(select_com,)][index]:\n",
    "\t\t\t\t\t\t\tv_p_num += 1\n",
    "\t\t\t\t\t\t\tdelta_com = []\n",
    "\t\t\t\t\t\t\tfor c in cmr:\n",
    "\t\t\t\t\t\t\t\tdelta_com.append(followup_separated[c][index] - source_features[index])\n",
    "\t\t\t\t\t\t\tcombined = sum(delta_com)\n",
    "\t\t\t\t\t\t\tdelta = followup_separated[select_com][index] - source_features[index]\n",
    "\t\t\t\t\t\t\td += np.linalg.norm(combined) - np.linalg.norm(delta)\n",
    "\n",
    "\t\t\t\t\t\t\tif index in failures_all[model_name][(select_com,)]:\n",
    "\t\t\t\t\t\t\t\tf_c += 1\n",
    "\t\t\t\t\t\t\tif index in failures_all[model_name][cmr]:\n",
    "\t\t\t\t\t\t\t\tf += 1\n",
    "\t\t\t\t\tdelta_d.append(d / v_p_num)\n",
    "\t\t\t\t\tdelta_e.append((f - f_c) / v_p_num)\n",
    "\t\t\t\tdeltadis.append(sum(delta_d) / len(delta_d))\n",
    "\t\t\t\tdeltaeff.append(sum(delta_e) / len(delta_e))\n",
    "\t\t\tcorrelation, p_value = spearmanr(deltadis, deltaeff)\n",
    "\t\t\tp_value_str = re.sub(r'e([+-])0+(\\d+)', r'e\\1\\2', f'{p_value:.1e}')\n",
    "\t\t\tprint(f\"& {i} & {correlation:.2f} & {p_value_str}\")\n",
    "\t\t\tplt.figure(figsize=(4, 3.5))\n",
    "\t\t\tplt.scatter(deltadis, deltaeff,  color=\"#002EA6\", s=15)\n",
    "\t\t\tplt.xlabel(r\"$\\Delta_M$\")\n",
    "\t\t\tplt.ylabel(r\"$\\mathit{DFR}_{\\mathit{best}}$\")\n",
    "\n",
    "\t\t\tplt.tight_layout()\n",
    "\t\t\tplt.savefig(f\"figures/RQ3.2/failure_{dataset}_{model}_{i}_scatter_plot.pdf\", dpi=300, bbox_inches='tight')\n",
    "\t\t\t# plt.show()\n",
    "\t\t\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check delta Effectiveness w.r.t. Fault\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import spearmanr\n",
    "import re\n",
    "\n",
    "for dataset in datasets:\n",
    "\tvalidity_followup = validity_all[dataset]\n",
    "\tif dataset == 'MNIST':\n",
    "\t\textractor = 'lenet5'\n",
    "\telif dataset == 'UTKFace':\n",
    "\t\textractor = 'insightface'\n",
    "\telse:\n",
    "\t\textractor = 'vgg16'\n",
    "\tsave_path = os.path.join('results', 'features', extractor, f'{dataset}.pt')\n",
    "\tsource__features = torch.load(save_path)\n",
    "\tfollowup__features_followup = []\n",
    "\tfor i in range(mr_num):\n",
    "\t\tsave_path = os.path.join('results', 'features', extractor, f'{dataset}_{i}.pt')\n",
    "\t\tfollowup__features_followup.append(torch.load(save_path))\n",
    "\tall_features = [source__features] + followup__features_followup\n",
    "\tall_features = [f.cpu().numpy() for f in all_features]\n",
    "\tall_features = np.concatenate(all_features, axis=0)\n",
    "\tpca = PCA(n_components=pca_n_components)\n",
    "\treduced_features = pca.fit_transform(all_features)\n",
    "\tsource_length = len(source__features)\n",
    "\tsource_features = reduced_features[:source_length]\n",
    "\tfollowup_features = reduced_features[source_length:]\n",
    "\tfollowup_separated = [followup_features[i * source_length:(i + 1) * source_length] for i in range(7)]\n",
    "\n",
    "\tfor model in models[dataset]:\n",
    "\t\tprint(dataset, model)\n",
    "\t\tmodel_name = get_model_name(dataset, model)\n",
    "\t\tfor i in range(2, strength_max+1):\n",
    "\t\t\tdeltadis, deltaeff = [], []\n",
    "\t\t\tfor com in combinations(range(mr_num), i):\n",
    "\t\t\t\tselect_com = com[0]\n",
    "\t\t\t\tfor j in range(1, len(com)):\n",
    "\t\t\t\t\tif (len(set(faults_all[model_name][(com[j],)].values())) > len(set(faults_all[model_name][(select_com,)].values()))):\n",
    "\t\t\t\t\t\tselect_com = com[j]\n",
    "\n",
    "\t\t\t\tdelta_d = []\n",
    "\t\t\t\tdelta_e = []\n",
    "\t\t\t\tfor cmr in permutations(com):\n",
    "\t\t\t\t\td, f, f_c, v_p_num = 0, set(), set(), 0\n",
    "\t\t\t\t\tfor index in range(source_length):\n",
    "\t\t\t\t\t\tif validity_followup[cmr][index] and validity_followup[(select_com,)][index]:\n",
    "\t\t\t\t\t\t\tv_p_num += 1\n",
    "\n",
    "\t\t\t\t\t\t\tdelta_com = []\n",
    "\t\t\t\t\t\t\tfor c in cmr:\n",
    "\t\t\t\t\t\t\t\tdelta_com.append(followup_separated[c][index] - source_features[index])\n",
    "\t\t\t\t\t\t\tcombined = sum(delta_com)\n",
    "\t\t\t\t\t\t\tdelta = followup_separated[select_com][index] - source_features[index]\n",
    "\t\t\t\t\t\t\td += np.linalg.norm(combined) - np.linalg.norm(delta)\n",
    "\n",
    "\t\t\t\t\t\t\t#print(cmr, d, index)\n",
    "\t\t\t\t\t\t\tif index in failures_all[model_name][(select_com,)]:\n",
    "\t\t\t\t\t\t\t\tf_c.add(faults_all[model_name][(select_com,)][index])\n",
    "\t\t\t\t\t\t\tif index in failures_all[model_name][cmr]:\n",
    "\t\t\t\t\t\t\t\tf.add(faults_all[model_name][cmr][index])\n",
    "\t\t\t\t\tdelta_d.append(d / v_p_num)\n",
    "\t\t\t\t\tdelta_e.append((len(f) - len(f_c)))\n",
    "\t\t\t\t#print(com, select_com, delta_d, delta_e)\n",
    "\t\t\t\tdeltadis.append(sum(delta_d) / len(delta_d))\n",
    "\t\t\t\tdeltaeff.append(sum(delta_e) / len(delta_e) / total_fault_num_all[model_name])\n",
    "\t\t\tcorrelation, p_value = spearmanr(deltadis, deltaeff)\n",
    "\t\t\tp_value_str = re.sub(r'e([+-])0+(\\d+)', r'e\\1\\2', f'{p_value:.1e}')\n",
    "\t\t\tprint(f\"& {i} & {correlation:.2f} & {p_value:.1e}\")\n",
    "\t\t\tplt.figure(figsize=(4, 3.5))\n",
    "\t\t\tplt.scatter(deltadis, deltaeff,  color=\"#002EA6\", s=15)\n",
    "\t\t\tplt.xlabel(r\"$\\Delta_M$\")\n",
    "\t\t\tplt.ylabel(r\"$\\mathit{DFT}_{\\mathit{best}}$\")\n",
    "\n",
    "\t\t\tplt.tight_layout()\n",
    "\t\t\tplt.savefig(f\"figures/RQ3.2/fault_{dataset}_{model}_{i}_scatter_plot.pdf\", dpi=300, bbox_inches='tight')\n",
    "\t\t\t# plt.show()\n",
    "\t\t\tplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ3.3 Sequence\n",
    "\n",
    "How do the different composition sequences influence the test effectiveness of CMRs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_effect(dataset, model):\n",
    "\tmodel_name = get_model_name(dataset, model)\n",
    "\tprint(model_name)\n",
    "\tsource_num = test_cases_num[dataset]\n",
    "\tfor k in range(1, strength_max):\n",
    "\t\tk = k + 1\n",
    "\t\tdfr_max_mean, dfr_max_std, dfr_mean_mean, dfr_mean_std = [], [], [], []\n",
    "\t\tdft_max_mean, dft_max_std, dft_mean_mean, dft_mean_std = [], [], [], []\n",
    "\t\tfor com in combinations(range(mr_num), k):\n",
    "\t\t\tdfr_max_com, dfr_mean_com, dft_max_com, dft_mean_com = [], [], [], []\n",
    "\t\t\tfr_max, fr_mean = 0, 0\n",
    "\t\t\tft_max, ft_mean = 0, 0\n",
    "\t\t\tfor mr in com:\n",
    "\t\t\t\tfr_tmp = len(failures_all[model_name][(mr,)]) / source_num\n",
    "\t\t\t\tfr_mean += fr_tmp\n",
    "\t\t\t\tif fr_tmp > fr_max:\n",
    "\t\t\t\t\tfr_max = fr_tmp\n",
    "\t\t\t\tft_tmp = len(set(faults_all[model_name][(mr,)].values())) / total_fault_num_all[model_name]\n",
    "\t\t\t\tft_mean += ft_tmp\n",
    "\t\t\t\tif ft_tmp > ft_max:\n",
    "\t\t\t\t\tft_max = ft_tmp\n",
    "\t\t\tfr_mean, ft_mean = fr_mean / len(com), ft_mean / len(com)\n",
    "\t\t\tfor per in permutations(com):\n",
    "\t\t\t\tfr = len(failures_all[model_name][per]) / source_num\n",
    "\t\t\t\tft = len(set(faults_all[model_name][per].values())) / total_fault_num_all[model_name]\n",
    "\t\t\t\tdfr_max_com.append(fr - fr_max)\n",
    "\t\t\t\tdfr_mean_com.append(fr - fr_mean)\n",
    "\t\t\t\tdft_max_com.append(ft - ft_max)\n",
    "\t\t\t\tdft_mean_com.append(ft - ft_mean)\n",
    "\n",
    "\t\t\tdfr_max_mean.append(np.mean(dfr_max_com))\n",
    "\t\t\tdfr_max_std.append(np.std(dfr_max_com))\n",
    "\t\t\tdfr_mean_mean.append(np.mean(dfr_mean_com))\n",
    "\t\t\tdfr_mean_std.append(np.std(dfr_mean_com))\n",
    "\t\t\tdft_max_mean.append(np.mean(dft_max_com))\n",
    "\t\t\tdft_max_std.append(np.std(dft_max_com))\n",
    "\t\t\tdft_mean_mean.append(np.mean(dft_mean_com))\n",
    "\t\t\tdft_mean_std.append(np.std(dft_mean_com))\n",
    "\n",
    "\t\tdfr_max_mean, dfr_max_std, dfr_mean_mean, dfr_mean_std = np.mean(dfr_max_mean), np.mean(dfr_max_std), np.mean(dfr_mean_mean), np.mean(dfr_mean_std)\n",
    "\t\tdft_max_mean, dft_max_std, dft_mean_mean, dft_mean_std = np.mean(dft_max_mean), np.mean(dft_max_std), np.mean(dft_mean_mean), np.mean(dft_mean_std)\n",
    "\t\tprint(f\"& {k} & {dfr_max_mean*100:.2f}\\\\% & {dfr_max_std*100:.2f}\\\\% & {dfr_mean_mean*100:.2f}\\\\% & {dfr_mean_std*100:.2f}\\\\% & \"\n",
    "\t\t\tf\"{dft_max_mean*100:.2f}\\\\% & {dft_max_std*100:.2f}\\\\% & {dft_mean_mean*100:.2f}\\\\% & {dft_mean_std*100:.2f}\\\\% \\\\\\\\\")\n",
    "\n",
    "def compare_sequence():\n",
    "\tfor dataset in datasets:\n",
    "\t\tfor _, model in enumerate(models[dataset]):\n",
    "\t\t\tsequence_effect(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ4 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected CMRs and selected image indices\n",
    "selected_cmr_num = 50\n",
    "selected_source_num = 1000\n",
    "selected_cmrs_all = {}\n",
    "selected_indices_all = {}\n",
    "for model_name in model_names:\n",
    "\ttry:\n",
    "\t\tprint(model_name, end=' ')\n",
    "\t\twith open(f'results/samples/{model_name}_cmr{selected_cmr_num}.pkl', 'rb') as f:\n",
    "\t\t\tselected_cmrs = pickle.load(f)\n",
    "\t\tselected_cmrs_all[model_name] = selected_cmrs\n",
    "\t\tif selected_source_num == 0:\n",
    "\t\t\tselected_indices = list(range(test_cases_num[model_name.split('_')[0]]))\n",
    "\t\telse:\n",
    "\t\t\twith open(f'results/samples/{model_name.split(\"_\")[0]}_{selected_source_num}.pkl', 'rb') as f:\n",
    "\t\t\t\tselected_indices = pickle.load(f)\n",
    "\t\tselected_indices_all[model_name.split('_')[0]] = selected_indices\n",
    "\t\tprint(f\"done, {len(selected_indices)=}, {selected_cmrs.keys()=}.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load failure and fault for selected image indices for original models\n",
    "selected_failures_all, selected_faults_all = {}, {}\n",
    "for model_name in model_names:\n",
    "\ttry:\n",
    "\t\tprint(model_name, end=' ')\n",
    "\t\twith open(f'results/errors/failure_{model_name}_{selected_source_num}.pkl', 'rb') as f:\n",
    "\t\t\tfailure = pickle.load(f)\n",
    "\t\tselected_failures_all[model_name] = failure\n",
    "\t\twith open(f'results/errors/fault_{model_name}_{selected_source_num}.pkl', 'rb') as f:\n",
    "\t\t\tfaults = pickle.load(f)\n",
    "\t\tselected_faults_all[model_name] = faults\n",
    "\t\tprint(f\"done, {len(failure)=}, {len(faults)=}.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented model results\n",
    "aug_failures_all, aug_faults_all = {}, {}\n",
    "for model_name in augmented_model_names:\n",
    "\ttry:\n",
    "\t\tprint(model_name, end=' ')\n",
    "\t\twith open(f'results/errors/failure_{model_name}_{selected_source_num}.pkl', 'rb') as f:\n",
    "\t\t\tfailure = pickle.load(f)\n",
    "\t\taug_failures_all[model_name] = failure\n",
    "\t\twith open(f'results/errors/fault_{model_name}_{selected_source_num}.pkl', 'rb') as f:\n",
    "\t\t\tfaults = pickle.load(f)\n",
    "\t\taug_faults_all[model_name] = faults\n",
    "\t\tprint(f\"done, {len(failure)=}, {len(faults)=}.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the total failure ratio and the total number of faults\n",
    "def compare_selected_cmrs(model_name, cmrs, total_fault_num):\n",
    "\tdataset = model_name.split('_')[0]\n",
    "\tsource_num = len(selected_indices_all[dataset])\n",
    "\tcmr_num = 0\n",
    "\tfailure_CMR, failure_MR_MAX, failure_MR_MEAN = 0, 0, 0\n",
    "\tfault_CMR, fault_MR_MAX, fault_MR_MEAN = 0, 0, 0\n",
    "\n",
    "\tif 'Aug' in model_name:\n",
    "\t\tfailures, faults = aug_failures_all[model_name], aug_faults_all[model_name]\n",
    "\telse:\n",
    "\t\tfailures, faults = selected_failures_all[model_name], selected_faults_all[model_name]\n",
    "\n",
    "\tfor cmr in cmrs:\n",
    "\t\ti = len(cmr)\n",
    "\t\tcmr_num += 1\n",
    "\t\tfailure_com, fault_com = [], []\n",
    "\t\tfault_com_union = set()\n",
    "\t\tfor com in cmr:\n",
    "\t\t\tfailure_com.append(len(failures[(com,)]))\n",
    "\t\t\tfault_com.append(len(set(faults[(com,)].values())))\n",
    "\t\t\tfault_com_union.update(set(faults[(com,)].values()))\n",
    "\n",
    "\t\tfailure_CMR += len(failures[cmr]) / source_num\n",
    "\t\tfailure_MR_MAX += max(failure_com) / source_num\n",
    "\t\tfailure_MR_MEAN += sum(failure_com) / i / source_num\n",
    "\n",
    "\t\tfault_CMR += len(set(faults[cmr].values())) / total_fault_num\n",
    "\t\tfault_MR_MAX += max(fault_com) / total_fault_num\n",
    "\t\tfault_MR_MEAN += len(fault_com_union) / i / total_fault_num\n",
    "\n",
    "\tfailure_cmr = failure_CMR / cmr_num\n",
    "\tfailure_mr_max, failure_mr_mean = failure_MR_MAX / cmr_num, failure_MR_MEAN / cmr_num\n",
    "\tfault_cmr = fault_CMR / cmr_num\n",
    "\tfault_mr_max, fault_mr_mean = fault_MR_MAX / cmr_num, fault_MR_MEAN / cmr_num\n",
    "\n",
    "\treturn [\n",
    "\t\tfailure_cmr, failure_mr_max, failure_mr_mean,\n",
    "\t\tfault_cmr, fault_mr_max, fault_mr_mean\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "compare_aug_method = [None, \"online\"]\n",
    "df = pd.DataFrame(columns=[\n",
    "\t'Dataset', 'Model', 'Augmentation',\n",
    "\t'failure_mr_max', 'failure_mr_mean', 'failure_cmr',\n",
    "\t'fault_mr_max', 'fault_mr_mean', 'fault_cmr',\n",
    "])\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\tfor m in [f'random_{i}' for i in range(5)]:\n",
    "\t\t\tselected_cmrs = selected_cmrs_all[get_model_name(dataset, model)][m]\n",
    "\t\t\ttotal_fault_num = 0\n",
    "\t\t\t# print(selected_cmrs)\n",
    "\t\t\tfor aug in compare_aug_method:\n",
    "\t\t\t\tmodel_name = get_model_name(dataset, model, augment=aug)\n",
    "\t\t\t\tif not aug:\n",
    "\t\t\t\t\ttotal_fault_num += len(set.union(*[set(f) for k, f in faults_all[model_name].items() if k in selected_cmrs]))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttotal_fault_num += len(set.union(*[set(f) for k, f in aug_faults_all[model_name].items() if k in selected_cmrs]))\n",
    "\t\t\tfor aug in compare_aug_method:\n",
    "\t\t\t\tmodel_name = get_model_name(dataset, model, augment=aug)\n",
    "\t\t\t\t# print(f\"{model_name:<29}\", f\"{m:<12}\", end='\\t')\n",
    "\t\t\t\tres = compare_selected_cmrs(model_name, selected_cmrs, total_fault_num)\n",
    "\t\t\t\tdf.loc[len(df)] = [\n",
    "\t\t\t\t\tdataset, model, ('None' if not aug else aug),\n",
    "\t\t\t\t\tres[1] * 100, res[2] * 100, res[0] * 100,\n",
    "\t\t\t\t\tres[4] * 100, res[5] * 100, res[3] * 100,\n",
    "\t\t\t\t]\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df.groupby(['Dataset', 'Model', 'Augmentation']).mean().sort_values(by=['Dataset'], key=lambda x: [datasets.index(i) for i in x]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
