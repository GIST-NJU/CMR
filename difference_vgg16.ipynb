{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib .pyplot as plt\n",
    "from itertools import permutations, combinations\n",
    "import os\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rotate(x, degree):\n",
    "    # Rotate the image by degrees counter clockwise\n",
    "    return x.rotate(degree)\n",
    "\n",
    "def enh_bri(x, brightness):\n",
    "    bri = ImageEnhance.Brightness(x)\n",
    "    return bri.enhance(brightness)\n",
    "\n",
    "def enh_con(x, contrast):\n",
    "    con = ImageEnhance.Contrast(x)\n",
    "    return con.enhance(contrast)\n",
    "\n",
    "def enh_sha(x, sharpness):\n",
    "    sha = ImageEnhance.Sharpness(x)\n",
    "    return sha.enhance(sharpness)\n",
    "\n",
    "def gaussian(x, kernel_size):\n",
    "    x = np.array(x)\n",
    "    x = cv.GaussianBlur(x, kernel_size, sigmaX=0)\n",
    "    return Image.fromarray(x)\n",
    "\n",
    "def shear(x, shear_factor):\n",
    "    # 定义错切变换矩阵\n",
    "    shear_matrix = [1, shear_factor, 0, 0, 1, 0]\n",
    "\n",
    "    # 创建Affine对象并应用错切变换\n",
    "    sheared_img = x.transform(\n",
    "        x.size, Image.Transform.AFFINE, shear_matrix\n",
    "    )\n",
    "    return sheared_img\n",
    "\n",
    "def translate(x, shift):\n",
    "    shift_x, shift_y = shift[0], shift[1]\n",
    "    # 进行平移操作\n",
    "    translated_img = x.transform(\n",
    "        x.size, Image.Transform.AFFINE, (1, 0, shift_x, 0, 1, shift_y)\n",
    "    )\n",
    "    return translated_img\n",
    "\n",
    "\n",
    "mrs = [rotate, enh_bri, enh_sha, enh_con, gaussian, shear, translate]\n",
    "mrs_name =[mr.__name__ for mr in mrs]\n",
    "paras = [3, 0.8, 0.8, 0.8, (3, 3), 0.1, (1,1)]\n",
    "paras_more = [[6.5], \n",
    "              [ 0.35],\n",
    "              [0.35],\n",
    "              [1.8, 1.7, 1.6, 1.5, 1.4, 1.3, 1.2 , 1.1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "              [(9,9)], [0.35], [(6,6), (7,7)]]\n",
    "datasets = ['MNIST', 'Caltech256', 'VOC', 'COCO']\n",
    "models = {'MNIST': [ 'ResNet18'],\n",
    "          'Caltech256': ['DenseNet121'],\n",
    "          'VOC': ['MSRN'],\n",
    "          'COCO': ['MLD']}\n",
    "model_names = ['MNIST_AlexNet_9938', 'MNIST_ResNet18_9906','Caltech256_DenseNet121_6838', 'Caltech256_Inception_v3_6720',\n",
    "               'VOC_MSRN', 'VOC_MCAR', 'COCO_MLD', 'COCO_ASL']\n",
    "\n",
    "def get_model_name(dataset, model):\n",
    "    for i in range(len(model_names)):\n",
    "        if dataset+'_'+model in model_names[i]:\n",
    "            return model_names[i]\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, cmr=None, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for idx, (img, label) in enumerate(dataset):\n",
    "            if cmr is not None:\n",
    "                for index in cmr:\n",
    "                    img = mrs[index](img, paras[index])\n",
    "            self.data.append(img)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data, []\n",
    "    \n",
    "class CustomDatasetSingleMR(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, mr, para, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for idx, (img, label) in enumerate(dataset):\n",
    "            img = mr(img, para)\n",
    "            self.data.append(img)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data, []\n",
    "\n",
    "coco_info =  {\n",
    "    \"data_name\": \"coco\",\n",
    "    \"data\": os.path.join(\"data\", \"COCO\"),\n",
    "    \"annotation_file\": os.path.join(\"data\", \"COCO\", 'annotations', 'image_info_test2014.json'),\n",
    "    \"phase\": \"test\",\n",
    "    \"num_classes\": 80\n",
    "}\n",
    "\n",
    "\n",
    "def load_testset(dataset_name, transform = None):\n",
    "\tif dataset_name == 'MNIST':\n",
    "\t\tdataset = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\telif dataset_name == 'Caltech256':\n",
    "\t\tcaltech256_dataset = torchvision.datasets.Caltech256(root='data', download=True)\n",
    "\t\tX = [caltech256_dataset[i][0] for i in range(len(caltech256_dataset))]\n",
    "\t\ty = [caltech256_dataset[i][1] for i in range(len(caltech256_dataset))]\n",
    "\t\t_, X_test, _, y_test = train_test_split(X, y, test_size=0.1, random_state=18, stratify=y)\n",
    "\t\tdataset = CustomDataset(list(zip(X_test, y_test)), transform=transform)\n",
    "\telif dataset_name == 'VOC':\n",
    "\t\tdataset = torchvision.datasets.VOCDetection('data/VOC', year=\"2007\", image_set='test', transform=transform)\n",
    "\telse: #'COCO'\n",
    "\t\tdataset = torchvision.datasets.CocoDetection(os.path.join(coco_info['data'],'{}2014'.format(coco_info['phase'])), \n",
    "\t\t\tannFile=coco_info['annotation_file'], transform=transform)\n",
    "\t#print(dataset_name, type(dataset), len(dataset))\n",
    "\treturn dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1, vec2 = vec1.to(device), vec2.to(device)\n",
    "    dot_product = torch.dot(vec1, vec2)\n",
    "    norm1 = torch.norm(vec1)\n",
    "    norm2 = torch.norm(vec2)\n",
    "    r = dot_product / (norm1 * norm2)\n",
    "    return r.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['Caltech256']\n",
    "batch_size = 512\n",
    "\n",
    "transform_predict = transforms.Compose([\n",
    "\ttransforms.Resize(256),\n",
    "\ttransforms.CenterCrop(224),\n",
    "\ttransforms.Grayscale(num_output_channels=3), # the mode of some images is L not RGB\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def collate_fn_empty_labels(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = [[] for _ in batch]\n",
    "    return images, labels\n",
    "\n",
    "transform_vgg16 = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # VGG16需要224x224输入\n",
    "        transforms.Grayscale(num_output_channels=3),  # 单通道转RGB\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet统计量\n",
    "    ])\n",
    "\n",
    "transform_resnet = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(0, 1, 2, 3, 4) 5.72 0.8747163963391866\n",
      "(0, 1, 2, 3, 5) 5.92 0.8671289987032105\n",
      "(0, 1, 2, 3, 6) 4.76 0.9124159411408862\n",
      "(0, 1, 2, 4, 3) 5.73 0.87445486442827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     42\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m cmr \u001b[38;5;129;01min\u001b[39;00m permutations(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(mrs)), k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     43\u001b[0m \t\t\u001b[38;5;66;03m# vgg16 extract features of follow-up images\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \t\tfollowuptestset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_resnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \t\tloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(followuptestset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn_empty_labels, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     46\u001b[0m \t\tfeatures_followup \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[29], line 88\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, dataset, cmr, transform)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m cmr:\n\u001b[0;32m---> 88\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mmrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m, in \u001b[0;36menh_bri\u001b[0;34m(x, brightness)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menh_bri\u001b[39m(x, brightness):\n\u001b[1;32m     25\u001b[0m     bri \u001b[38;5;241m=\u001b[39m ImageEnhance\u001b[38;5;241m.\u001b[39mBrightness(x)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrightness\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/ImageEnhance.py:36\u001b[0m, in \u001b[0;36m_Enhance.enhance\u001b[0;34m(self, factor)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance\u001b[39m(\u001b[38;5;28mself\u001b[39m, factor):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Returns an enhanced image.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    :rtype: :py:class:`~PIL.Image.Image`\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegenerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:3340\u001b[0m, in \u001b[0;36mblend\u001b[0;34m(im1, im2, alpha)\u001b[0m\n\u001b[1;32m   3338\u001b[0m im1\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   3339\u001b[0m im2\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m-> 3340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im1\u001b[38;5;241m.\u001b[39m_new(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate the distance of k-CMRs for (k from 1 to 5) through Resnet50\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\t\n",
    "\t\t# validity\n",
    "\t\tvalidity_filename = f'results/SelfOracle/{dataset}_validity.npy'\n",
    "\t\tvalidity = np.load(validity_filename, allow_pickle=True).item()\n",
    "\t\tfilename_threshold = f'results/SelfOracle/{dataset}_threshold.txt'\n",
    "\t\twith open(filename_threshold) as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\t\tthreshold = float(lines[1].split(':')[1].strip())\n",
    "\n",
    "\t\tweights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "\t\tresnet = torchvision.models.resnet50(weights=weights).to(device)\n",
    "\t\tresnet.eval()\n",
    "\n",
    "\t\ttransform_resnet = transforms.Compose([\n",
    "\t\t\ttransforms.Lambda(lambda x: x.convert('RGB') if hasattr(x, 'mode') and x.mode != 'RGB' else x),\n",
    "\t\t\tweights.transforms(),\n",
    "\t\t])\n",
    "\n",
    "\t\t# 移除最后的全连接层，仅保留卷积+池化部分\n",
    "\t\tfeature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "\t\t#extract features of source images\n",
    "\t\tsourcetestset = load_testset(dataset, transform=transform_resnet)\n",
    "\t\tloader = torch.utils.data.DataLoader(sourcetestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\tfeatures_source = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, _ in loader:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tfeatures = feature_extractor(images)\n",
    "\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\tfeatures_source.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\tfeatures_source = torch.cat(features_source, dim=0)\n",
    "\n",
    "\t\tdistance = [{} for _ in range(5)]\n",
    "\n",
    "\t\ttest_dataset = load_testset(dataset)\n",
    "\t\tfor k in range(4, 5):\n",
    "\t\t\tfor cmr in permutations(range(len(mrs)), k+1):\n",
    "\t\t\t\t# vgg16 extract features of follow-up images\n",
    "\t\t\t\tfollowuptestset = CustomDataset(test_dataset, cmr=cmr, transform=transform_resnet)\n",
    "\t\t\t\tloader = torch.utils.data.DataLoader(followuptestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\t\t\tfeatures_followup = []\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tfor images, _ in loader:\n",
    "\t\t\t\t\t\timages = images.to(device)\n",
    "\t\t\t\t\t\tfeatures = feature_extractor(images)\n",
    "\t\t\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\t\t\tfeatures_followup.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\t\t\t\tfeatures_followup = torch.cat(features_followup, dim=0)\n",
    "\n",
    "\t\t\t\tnum, eu_sum, cos_sum = 0, 0, 0\n",
    "\t\t\t\tfor i in range(len(features_followup)):\n",
    "\t\t\t\t\tif validity[cmr][i] <= threshold:\n",
    "\t\t\t\t\t\tnum += 1\n",
    "\t\t\t\t\t\teu_sum +=  torch.sqrt(torch.sum((features_source[i] - features_followup[i]) ** 2)).cpu().item()\n",
    "\t\t\t\t\t\tcos_sum += cosine_similarity(features_source[i], features_followup[i])\n",
    "\t\t\t\tdistance[k][cmr] = eu_sum / num\n",
    "\t\t\t\tprint(cmr, round(distance[k][cmr],2), cos_sum/num)\n",
    "\n",
    "\t\tnp.save(f\"results/resnet50/cmr_{model}\", np.array(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance of k-CMRs for (k from 1 to 5) through Vgg16\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\t\n",
    "\t\t# validity\n",
    "\t\tvalidity_filename = f'results/SelfOracle/{dataset}_validity.npy'\n",
    "\t\tvalidity = np.load(validity_filename, allow_pickle=True).item()\n",
    "\t\tfilename_threshold = f'results/SelfOracle/{dataset}_threshold.txt'\n",
    "\t\twith open(filename_threshold) as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\t\tthreshold = float(lines[1].split(':')[1].strip())\n",
    "\n",
    "\t\tweights = torchvision.models.VGG16_Weights.DEFAULT\n",
    "\t\tvgg = torchvision.models.vgg16(weights=weights).features.to(device)\n",
    "\t\tvgg.eval()\n",
    "\n",
    "\t\t#vgg16 extract features of source images\n",
    "\t\tsourcetestset = load_testset(dataset, transform=transform_vgg16)\n",
    "\t\tloader = torch.utils.data.DataLoader(sourcetestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\tfeatures_source = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, _ in loader:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tfeatures = vgg(images)\n",
    "\t\t\t\tfeatures = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\tfeatures_source.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\tfeatures_source = torch.cat(features_source, dim=0)\n",
    "\n",
    "\t\tdistance = [{} for _ in range(5)]\n",
    "\n",
    "\t\ttest_dataset = load_testset(dataset)\n",
    "\t\tfor k in range(5):\n",
    "\t\t\tfor cmr in permutations(range(len(mrs)), k+1):\n",
    "\t\t\t\tprint(cmr)\n",
    "\t\t\t\t# vgg16 extract features of follow-up images\n",
    "\t\t\t\tfollowuptestset = CustomDataset(test_dataset, cmr=cmr, transform=transform_vgg16)\n",
    "\t\t\t\tloader = torch.utils.data.DataLoader(followuptestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\t\t\tfeatures_followup = []\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tfor images, _ in loader:\n",
    "\t\t\t\t\t\timages = images.to(device)\n",
    "\t\t\t\t\t\tfeatures = vgg(images)\n",
    "\t\t\t\t\t\tfeatures = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "\t\t\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\t\t\tfeatures_followup.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\t\t\t\tfeatures_followup = torch.cat(features_followup, dim=0)\n",
    "\n",
    "\t\t\t\tnum, eu_sum = 0, 0\n",
    "\t\t\t\tfor i in range(len(features_followup)):\n",
    "\t\t\t\t\tif validity[cmr][i] <= threshold:\n",
    "\t\t\t\t\t\tnum += 1\n",
    "\t\t\t\t\t\teu_sum +=  torch.sqrt(torch.sum((features_source[i] - features_followup[i]) ** 2)).cpu().item()\n",
    "\t\t\t\t\t\t#cos_sum += cosine_similarity(features_source[i], features_followup[i])\n",
    "\t\t\t\tdistance[k][cmr] = eu_sum / num\n",
    "\n",
    "\t\tnp.save(f\"results/vgg16/cmr_{model}\", np.array(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.992199831228007\n",
      "Caltech256_DenseNet121_6838\n",
      "275.5696031746032\n",
      "291\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Caltech256'\n",
    "model = 'DenseNet121'\n",
    "model_name = get_model_name(dataset, model)\n",
    "d = np.load(f\"results/vgg16/cmr_{model}.npy\", allow_pickle=True) \n",
    "print(sum(d[4].values())/ len(d[4]))\n",
    "\n",
    "\n",
    "print(model_name)\n",
    "with open(f'error_revealing/error_{model_name}.pkl', 'rb') as f:\n",
    "    error = pickle.load(f)\n",
    "fault = np.load(os.path.join('results', 'error_classification', dataset+'_'+model+'.npy'))\n",
    "\n",
    "def get_fault(failure_list, fault):\n",
    "    f = set()\n",
    "    for failure in failure_list:\n",
    "        if fault[failure] != -1:\n",
    "            f.add(fault[failure])\n",
    "    return f\n",
    "\n",
    "fault_5_num =0\n",
    "fault_5 = set()\n",
    "cmr_5 = 0\n",
    "fault_5_max = 0\n",
    "\n",
    "for cmr in error:\n",
    "    if len(cmr) < 5:\n",
    "        continue\n",
    "    cmr_5 += 1\n",
    "    if len(get_fault(error[cmr], fault)) > fault_5_max:\n",
    "        fault_5_max = len(get_fault(error[cmr], fault))\n",
    "    fault_5.update(get_fault(error[cmr], fault))\n",
    "    fault_5_num += len(get_fault(error[cmr], fault))\n",
    "    \n",
    "print(fault_5_num / cmr_5)\n",
    "print(fault_5_max)\n",
    "print(len(fault_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "a = set()\n",
    "a.update([1,3,2])\n",
    "a.update([4,3,2])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "data = d\n",
    "\n",
    "# 将数据转换为长格式 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Group\": [i+1 for i in range(len(data)) for _ in data[i]],\n",
    "    \"Value\": [v for d in data for v in d.values()]\n",
    "})\n",
    "\n",
    "# 绘制箱线图\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x=\"Group\", y=\"Value\")\n",
    "\n",
    "# 添加标题\n",
    "plt.title(\"Box Plot of Dictionary Values (Seaborn)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(set_a, set_b):\n",
    "    \"\"\"\n",
    "    计算两个集合的Jaccard相似度（交集大小/并集大小）\n",
    "    \n",
    "    参数:\n",
    "        set_a (set): 第一个集合\n",
    "        set_b (set): 第二个集合\n",
    "    \n",
    "    返回:\n",
    "        float: Jaccard相似度，范围[0,1]\n",
    "    \"\"\"\n",
    "    # 计算交集和并集\n",
    "    intersection = set_a & set_b  # 或使用 set_a.intersection(set_b)\n",
    "    union = set_a | set_b         # 或使用 set_a.union(set_b)\n",
    "    \n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caltech256_DenseNet121_6838\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rotate_6.5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m valid_num\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pred_followup)):\n\u001b[0;32m---> 80\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalidity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmrs_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpara\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m     81\u001b[0m \t\tvalid_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     82\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m pred_source[i] \u001b[38;5;241m!=\u001b[39m pred_followup[i] \u001b[38;5;129;01mand\u001b[39;00m faults[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rotate_6.5'"
     ]
    }
   ],
   "source": [
    "# use vgg16\n",
    "\n",
    "datasets = ['Caltech256']\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\tmodel_name = get_model_name(dataset, model)\n",
    "\t\tprint(model_name)\n",
    "\t\tpred_source = np.load(f'predictions/{model_name}_source.npy')\n",
    "\t\t\n",
    "\t\t# validity\n",
    "\t\tvalidity_filename = f'results/increase_para/{dataset}.npy'\n",
    "\t\tvalidity = np.load(validity_filename, allow_pickle=True).item()\n",
    "\t\tfilename_threshold = f'results/SelfOracle/{dataset}_threshold.txt'\n",
    "\t\twith open(filename_threshold) as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\t\tthreshold = float(lines[1].split(':')[1].strip())\n",
    "\n",
    "\t\t# faults\n",
    "\t\tfaults = np.load(os.path.join('results', 'error_classification', dataset+'_'+model+'.npy'))\n",
    "\n",
    "\n",
    "\t\tmodel = torchvision.models.densenet121(weights='DEFAULT')  \n",
    "\t\tnum_classes = 257\n",
    "\t\tmodel.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "\t\tmodel.load_state_dict(torch.load('./models/'+model_name+'.pth'))\n",
    "\t\tmodel.to(device)\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\t#vgg16 extract features of source images\n",
    "\t\tweights = torchvision.models.VGG16_Weights.DEFAULT\n",
    "\t\tvgg = torchvision.models.vgg16(weights=weights).features.to(device)\n",
    "\t\tvgg.eval()\n",
    "\n",
    "\t\tsourcetestset = load_testset(dataset, transform=transform_vgg16)\n",
    "\t\tloader = torch.utils.data.DataLoader(sourcetestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\tfeatures_source = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, _ in loader:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tfeatures = vgg(images)\n",
    "\t\t\t\tfeatures = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\tfeatures_source.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\tfeatures_source = torch.cat(features_source, dim=0)\n",
    "\n",
    "\t\ttest_dataset = load_testset(dataset)\n",
    "\t\tfor index, mr in enumerate(mrs):\n",
    "\t\t\tfor para in  paras_more[index]:\n",
    "\n",
    "\t\t\t\t# predict\n",
    "\t\t\t\tdataset_followup = CustomDatasetSingleMR(test_dataset, mr=mr, para=para, transform=transform_predict)\n",
    "\t\t\t\ttestload_followup = torch.utils.data.DataLoader(dataset_followup, batch_size=batch_size, shuffle=False)\n",
    "\t\t\t\tpred_followup = np.zeros(len(pred_source), dtype=int)\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tfor i,(X,y) in enumerate(testload_followup):\n",
    "\t\t\t\t\t\tX = X.to(device)\n",
    "\t\t\t\t\t\toutputs = model(X)\n",
    "\t\t\t\t\t\t_, pred = torch.max(outputs, 1)\n",
    "\t\t\t\t\t\tpred_followup[i*batch_size:i*batch_size+X.size(0)] = pred.cpu()\n",
    "\n",
    "\t\t\t\t# vgg16 extract features of follow-up images\n",
    "\t\t\t\tfollowuptestset = CustomDatasetSingleMR(test_dataset, mr=mr, para=para, transform=transform_vgg16)\n",
    "\t\t\t\tloader = torch.utils.data.DataLoader(followuptestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\t\t\tfeatures_followup = []\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tfor images, _ in loader:\n",
    "\t\t\t\t\t\timages = images.to(device)\n",
    "\t\t\t\t\t\tfeatures = vgg(images)\n",
    "\t\t\t\t\t\tfeatures = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "\t\t\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\t\t\tfeatures_followup.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\t\t\t\tfeatures_followup = torch.cat(features_followup, dim=0)\n",
    "\n",
    "\t\t\t\t# calculate faults, the difference in embedding space\n",
    "\t\t\t\tfault_detected = set()\n",
    "\t\t\t\tnum, eu_sum, cos_sum = 0, 0, 0\n",
    "\t\t\t\tvalid_num= 0\n",
    "\t\t\t\tfor i in range(len(pred_followup)):\n",
    "\t\t\t\t\tif validity[f'{mrs_name[index]}_{para}'][i] <= threshold:\n",
    "\t\t\t\t\t\tvalid_num += 1\n",
    "\t\t\t\t\t\tif pred_source[i] != pred_followup[i] and faults[i] != -1:\n",
    "\t\t\t\t\t\t\tfault_detected.add(faults[i])\n",
    "\t\t\t\t\t\tnum += 1\n",
    "\t\t\t\t\t\teu_sum +=  torch.sqrt(torch.sum((features_source[i] - features_followup[i]) ** 2)).cpu().item()\n",
    "\t\t\t\t\t\tcos_sum += cosine_similarity(features_source[i], features_followup[i])\n",
    "\t\t\t\tiou = get_iou(fault_detected, fault_5)\t\n",
    "\n",
    "\t\t\t\tprint(f'{mrs_name[index]} {para}: Valid: {valid_num}\\tFaults: {len(fault_detected)}\\tIOU: {iou}\\tEU: {round(eu_sum/num, 2)}\\tCos: {round(cos_sum/num, 2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caltech256_DenseNet121_6838\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "rotate 3: Valid: 3061\tFaults: 186\tEU: 4.17\tCos: 0.93\n",
      "rotate 4: Valid: 3061\tFaults: 198\tEU: 4.65\tCos: 0.92\n",
      "rotate 5: Valid: 3061\tFaults: 207\tEU: 5.03\tCos: 0.9\n",
      "rotate 6: Valid: 3061\tFaults: 221\tEU: 5.38\tCos: 0.89\n",
      "rotate 7: Valid: 3061\tFaults: 231\tEU: 5.68\tCos: 0.88\n",
      "enh_bri 1.8: Valid: 3061\tFaults: 280\tEU: 5.36\tCos: 0.88\n",
      "enh_bri 1.7: Valid: 3061\tFaults: 272\tEU: 4.97\tCos: 0.9\n",
      "enh_bri 1.6: Valid: 3061\tFaults: 253\tEU: 4.54\tCos: 0.91\n",
      "enh_bri 1.5: Valid: 3061\tFaults: 244\tEU: 4.07\tCos: 0.93\n",
      "enh_bri 1.4: Valid: 3061\tFaults: 223\tEU: 3.53\tCos: 0.95\n",
      "enh_bri 1.3: Valid: 3061\tFaults: 193\tEU: 2.92\tCos: 0.96\n",
      "enh_bri 1.2: Valid: 3061\tFaults: 163\tEU: 2.19\tCos: 0.98\n",
      "enh_bri 1.1: Valid: 3061\tFaults: 109\tEU: 1.32\tCos: 0.99\n",
      "enh_bri 0.9: Valid: 3061\tFaults: 89\tEU: 0.99\tCos: 1.0\n",
      "enh_bri 0.8: Valid: 3061\tFaults: 131\tEU: 1.58\tCos: 0.99\n",
      "enh_bri 0.7: Valid: 3061\tFaults: 166\tEU: 2.11\tCos: 0.98\n",
      "enh_bri 0.6: Valid: 3061\tFaults: 205\tEU: 2.56\tCos: 0.97\n",
      "enh_bri 0.5: Valid: 3061\tFaults: 233\tEU: 2.84\tCos: 0.97\n",
      "enh_bri 0.4: Valid: 3061\tFaults: 255\tEU: 3.09\tCos: 0.96\n",
      "enh_bri 0.3: Valid: 3061\tFaults: 275\tEU: 3.32\tCos: 0.96\n",
      "enh_bri 0.2: Valid: 3061\tFaults: 302\tEU: 3.76\tCos: 0.95\n",
      "enh_bri 0.1: Valid: 3061\tFaults: 309\tEU: 4.7\tCos: 0.92\n",
      "enh_sha 1.8: Valid: 3061\tFaults: 119\tEU: 1.51\tCos: 0.99\n",
      "enh_sha 1.7: Valid: 3061\tFaults: 112\tEU: 1.38\tCos: 0.99\n",
      "enh_sha 1.6: Valid: 3061\tFaults: 96\tEU: 1.23\tCos: 0.99\n",
      "enh_sha 1.5: Valid: 3061\tFaults: 81\tEU: 1.07\tCos: 1.0\n",
      "enh_sha 1.4: Valid: 3061\tFaults: 67\tEU: 0.91\tCos: 1.0\n",
      "enh_sha 1.3: Valid: 3061\tFaults: 64\tEU: 0.75\tCos: 1.0\n",
      "enh_sha 1.2: Valid: 3061\tFaults: 43\tEU: 0.6\tCos: 1.0\n",
      "enh_sha 1.1: Valid: 3061\tFaults: 31\tEU: 0.48\tCos: 1.0\n",
      "enh_sha 0.9: Valid: 3061\tFaults: 38\tEU: 0.48\tCos: 1.0\n",
      "enh_sha 0.8: Valid: 3061\tFaults: 61\tEU: 0.62\tCos: 1.0\n",
      "enh_sha 0.7: Valid: 3061\tFaults: 77\tEU: 0.81\tCos: 1.0\n",
      "enh_sha 0.6: Valid: 3061\tFaults: 106\tEU: 1.03\tCos: 1.0\n",
      "enh_sha 0.5: Valid: 3061\tFaults: 127\tEU: 1.26\tCos: 0.99\n",
      "enh_sha 0.4: Valid: 3061\tFaults: 141\tEU: 1.49\tCos: 0.99\n",
      "enh_sha 0.3: Valid: 3061\tFaults: 163\tEU: 1.73\tCos: 0.99\n",
      "enh_sha 0.2: Valid: 3061\tFaults: 174\tEU: 1.96\tCos: 0.98\n",
      "enh_sha 0.1: Valid: 3061\tFaults: 190\tEU: 2.19\tCos: 0.98\n",
      "enh_con 1.8: Valid: 3061\tFaults: 232\tEU: 4.16\tCos: 0.93\n",
      "enh_con 1.7: Valid: 3061\tFaults: 227\tEU: 3.87\tCos: 0.94\n",
      "enh_con 1.6: Valid: 3061\tFaults: 214\tEU: 3.54\tCos: 0.95\n",
      "enh_con 1.5: Valid: 3061\tFaults: 202\tEU: 3.18\tCos: 0.96\n",
      "enh_con 1.4: Valid: 3061\tFaults: 177\tEU: 2.77\tCos: 0.97\n",
      "enh_con 1.3: Valid: 3061\tFaults: 157\tEU: 2.29\tCos: 0.98\n",
      "enh_con 1.2: Valid: 3061\tFaults: 130\tEU: 1.74\tCos: 0.99\n",
      "enh_con 1.1: Valid: 3061\tFaults: 81\tEU: 1.06\tCos: 1.0\n",
      "enh_con 0.9: Valid: 3061\tFaults: 59\tEU: 0.83\tCos: 1.0\n",
      "enh_con 0.8: Valid: 3061\tFaults: 111\tEU: 1.42\tCos: 0.99\n",
      "enh_con 0.7: Valid: 3061\tFaults: 150\tEU: 1.94\tCos: 0.99\n",
      "enh_con 0.6: Valid: 3061\tFaults: 188\tEU: 2.37\tCos: 0.98\n",
      "enh_con 0.5: Valid: 3061\tFaults: 226\tEU: 2.7\tCos: 0.97\n",
      "enh_con 0.4: Valid: 3061\tFaults: 252\tEU: 2.99\tCos: 0.97\n",
      "enh_con 0.3: Valid: 3061\tFaults: 272\tEU: 3.31\tCos: 0.96\n",
      "enh_con 0.2: Valid: 3061\tFaults: 292\tEU: 3.8\tCos: 0.94\n",
      "enh_con 0.1: Valid: 3061\tFaults: 307\tEU: 4.78\tCos: 0.91\n",
      "gaussian (3, 3): Valid: 3061\tFaults: 215\tEU: 2.81\tCos: 0.97\n",
      "gaussian (5, 5): Valid: 3061\tFaults: 257\tEU: 3.99\tCos: 0.93\n",
      "gaussian (7, 7): Valid: 3061\tFaults: 286\tEU: 5.5\tCos: 0.88\n",
      "gaussian (9, 9): Valid: 3061\tFaults: 301\tEU: 6.53\tCos: 0.84\n",
      "shear 0.1: Valid: 3061\tFaults: 189\tEU: 3.92\tCos: 0.94\n",
      "shear 0.2: Valid: 3061\tFaults: 234\tEU: 4.75\tCos: 0.91\n",
      "shear 0.3: Valid: 3061\tFaults: 266\tEU: 5.48\tCos: 0.89\n",
      "shear 0.4: Valid: 3061\tFaults: 282\tEU: 6.08\tCos: 0.86\n",
      "shear 0.5: Valid: 3061\tFaults: 298\tEU: 6.61\tCos: 0.84\n",
      "translate (1, 1): Valid: 3061\tFaults: 110\tEU: 2.02\tCos: 0.98\n",
      "translate (2, 2): Valid: 3061\tFaults: 133\tEU: 2.51\tCos: 0.98\n",
      "translate (3, 3): Valid: 3061\tFaults: 136\tEU: 2.81\tCos: 0.97\n",
      "translate (4, 4): Valid: 3061\tFaults: 158\tEU: 3.03\tCos: 0.96\n",
      "translate (5, 5): Valid: 3061\tFaults: 155\tEU: 3.22\tCos: 0.96\n"
     ]
    }
   ],
   "source": [
    "# use resnet50\n",
    "\n",
    "datasets = ['Caltech256']\n",
    "\n",
    "# transform_resnet = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize(256),\n",
    "#     torchvision.transforms.CenterCrop(224),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "for dataset in datasets:\n",
    "\tfor model in models[dataset]:\n",
    "\t\tmodel_name = get_model_name(dataset, model)\n",
    "\t\tprint(model_name)\n",
    "\t\tpred_source = np.load(f'predictions/{model_name}_source.npy')\n",
    "\t\t\n",
    "\t\t# validity\n",
    "\t\tvalidity_filename = f'results/increase_para/{dataset}.npy'\n",
    "\t\tvalidity = np.load(validity_filename, allow_pickle=True).item()\n",
    "\t\tfilename_threshold = f'results/SelfOracle/{dataset}_threshold.txt'\n",
    "\t\twith open(filename_threshold) as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\t\tthreshold = float(lines[1].split(':')[1].strip())\n",
    "\n",
    "\t\t# faults\n",
    "\t\tfaults = np.load(os.path.join('results', 'error_classification', dataset+'_'+model+'.npy'))\n",
    "\n",
    "\t\tmodel = torchvision.models.densenet121(weights='DEFAULT')  \n",
    "\t\tnum_classes = 257\n",
    "\t\tmodel.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "\t\tmodel.load_state_dict(torch.load('./models/'+model_name+'.pth'))\n",
    "\t\tmodel.to(device)\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\n",
    "\t\t# 使用 ResNet50 提取 source 图像特征\n",
    "\t\tweights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "\t\ttransform_resnet = transforms.Compose([\n",
    "\t\t\ttransforms.Lambda(lambda x: x.convert('RGB') if hasattr(x, 'mode') and x.mode != 'RGB' else x),\n",
    "\t\t\tweights.transforms(),\n",
    "\t\t])\n",
    "\t\tresnet = torchvision.models.resnet50(weights=weights).to(device)\n",
    "\t\tresnet.eval()\n",
    "\t\t# 移除最后的全连接层，仅保留卷积+池化部分\n",
    "\t\tfeature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])  # 去掉 fc 层\n",
    "\n",
    "\t\tsourcetestset = load_testset(dataset, transform=transform_resnet)\n",
    "\t\tloader = torch.utils.data.DataLoader(sourcetestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\tfeatures_source = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, _ in loader:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tfeatures = feature_extractor(images)\n",
    "\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\tfeatures_source.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\tfeatures_source = torch.cat(features_source, dim=0)\n",
    "\n",
    "\t\ttest_dataset = load_testset(dataset)\n",
    "\t\tfor index, mr in enumerate(mrs):\n",
    "\t\t\tfor para in  paras_more[index]:\n",
    "\n",
    "\t\t\t\t# predict\n",
    "\t\t\t\tdataset_followup = CustomDatasetSingleMR(test_dataset, mr=mr, para=para, transform=transform_predict)\n",
    "\t\t\t\ttestload_followup = torch.utils.data.DataLoader(dataset_followup, batch_size=batch_size, shuffle=False)\n",
    "\t\t\t\tpred_followup = np.zeros(len(pred_source), dtype=int)\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tfor i,(X,y) in enumerate(testload_followup):\n",
    "\t\t\t\t\t\tX = X.to(device)\n",
    "\t\t\t\t\t\toutputs = model(X)\n",
    "\t\t\t\t\t\t_, pred = torch.max(outputs, 1)\n",
    "\t\t\t\t\t\tpred_followup[i*batch_size:i*batch_size+X.size(0)] = pred.cpu()\n",
    "\n",
    "\t\t\t\t# vgg16 extract features of follow-up images\n",
    "\t\t\t\tfollowuptestset = CustomDatasetSingleMR(test_dataset, mr=mr, para=para, transform=transform_resnet)\n",
    "\t\t\t\tloader = torch.utils.data.DataLoader(followuptestset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=4)\n",
    "\t\t\t\tfeatures_followup = []\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tfor images, _ in loader:\n",
    "\t\t\t\t\t\timages = images.to(device)\n",
    "\t\t\t\t\t\tfeatures = feature_extractor(images)\n",
    "\t\t\t\t\t\tfeatures = features.view(features.size(0), -1)  # [batch, 512]\n",
    "\t\t\t\t\t\tfeatures_followup.append(features.cpu())  # 移回CPU节省显存\n",
    "\t\t\t\t\tfeatures_followup = torch.cat(features_followup, dim=0)\n",
    "\n",
    "\t\t\t\t# calculate faults, the difference in embedding space\n",
    "\t\t\t\tfault_detected = set()\n",
    "\t\t\t\tnum, eu_sum, cos_sum = 0, 0, 0\n",
    "\t\t\t\tvalid_num= 0\n",
    "\t\t\t\tfor i in range(len(pred_followup)):\n",
    "\t\t\t\t\tif validity[f'{mrs_name[index]}_{para}'][i] <= threshold:\n",
    "\t\t\t\t\t\tvalid_num += 1\n",
    "\t\t\t\t\t\tif pred_source[i] != pred_followup[i] and faults[i] != -1:\n",
    "\t\t\t\t\t\t\tfault_detected.add(faults[i])\n",
    "\t\t\t\t\t\tnum += 1\n",
    "\t\t\t\t\t\teu_sum +=  torch.sqrt(torch.sum((features_source[i] - features_followup[i]) ** 2)).cpu().item()\n",
    "\t\t\t\t\t\tcos_sum += cosine_similarity(features_source[i], features_followup[i])\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\tprint(f'{mrs_name[index]} {para}: Valid: {valid_num}\\tFaults: {len(fault_detected)}\\tEU: {round(eu_sum/num, 2)}\\tCos: {round(cos_sum/num, 2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance between sources and follow-up with larger paras\n",
    "def collate_fn_empty_labels(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = [[] for _ in batch]\n",
    "    return images, labels\n",
    "\n",
    "def extract_and_save_vgg16_features(datasetname, batch_size=256, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # VGG16需要224x224输入\n",
    "        transforms.Grayscale(num_output_channels=3),  # 单通道转RGB\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet统计量\n",
    "    ])\n",
    "    dataset = load_testset(datasetname, transform=transform)\n",
    "    \n",
    "    # 3. 加载VGG16模型(使用最新权重API)\n",
    "    weights = torchvision.models.VGG16_Weights.DEFAULT\n",
    "    model = torchvision.models.vgg16(weights=weights).features.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_empty_labels, num_workers=2)\n",
    "    \n",
    "    # 5. 特征提取\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "            features = features.view(features.size(0), -1)  # [batch, 512]\n",
    "            all_features.append(features.cpu())  # 移回CPU节省显存\n",
    "    # 6. 合并并保存特征\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    save_path = os.path.join('results', 'vgg16', f'{datasetname}.pt')\n",
    "    torch.save(all_features, save_path)\n",
    "    print(f\"Features saved to {save_path} | Shape: {all_features.shape}\")\n",
    "    \n",
    "for dataset in datasets:\n",
    "    extract_and_save_vgg16_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Vgg16 extract features and then calculate eu distance and cos similarity\n",
    "\n",
    "# increase the parameter of MR for Caltech256\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as ssim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1000\n",
    "\n",
    "transform_predict = transforms.Compose([\n",
    "\ttransforms.Resize(256),\n",
    "\ttransforms.CenterCrop(224),\n",
    "\ttransforms.Grayscale(num_output_channels=3), # the mode of some images is L not RGB\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_distance =  transforms.Compose([\n",
    "\ttransforms.Resize((64, 64)),\n",
    "\ttransforms.Grayscale(num_output_channels=3),\n",
    "\ttransforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_average_distance(mr, para, distance_total, ssim_total):\n",
    "\tdistance_sum, ssim_sum, num = 0, 0, 0\n",
    "\tfor i in range(len(distance_total)):\n",
    "\t\tif validity[f'{mr}_{para}'][i]<=threshold:\n",
    "\t\t\tdistance_sum += distance_total[i]\n",
    "\t\t\tssim_sum += ssim_total[i]\n",
    "\t\t\tnum += 1\n",
    "\treturn (distance_sum / num, ssim_sum / num) if num > 0 else (0, 0)\n",
    "\n",
    "def get_fault_num(mr, para, pred_source, pred_followup):\n",
    "\tfailure_index = []\n",
    "\tfor i in range(len(pred_source)):\n",
    "\t\t\tif validity[f'{mr}_{para}'][i]<=threshold and pred_source[i] != pred_followup[i]:\n",
    "\t\t\t\tfailure_index.append(i)\n",
    "\tcategory = set()\n",
    "\tfor failure in failure_index:\n",
    "\t\tif error_category[failure] != -1:\n",
    "\t\t\tcategory.add(error_category[failure])\n",
    "\treturn len(category)\n",
    "\n",
    "error_category = np.load(os.path.join('results', 'error_classification', 'Caltech256'+'_'+'DenseNet121'+'.npy'))\n",
    "\n",
    "model_name = 'Caltech256_DenseNet121_6838'\n",
    "model = torchvision.models.densenet121(weights='DEFAULT')  \n",
    "num_classes = 257\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "model.load_state_dict(torch.load('./models/'+model_name+'.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def calculate_euclidean(imgs1, imgs2):\n",
    "    return torch.norm(imgs1 - imgs2, p=2, dim=(1, 2, 3)).cpu().tolist()\n",
    "\n",
    "def calculate_ssim(imgs1, imgs2):\n",
    "      ssim_calculator = ssim(data_range=1.0, reduction='none').to(device)\n",
    "      return ssim_calculator(imgs1, imgs2).cpu().tolist()\n",
    "\n",
    "def calculate_diff(imgs1, imgs2): \n",
    "\teu = calculate_euclidean(imgs1, imgs2)\n",
    "\tss = calculate_ssim(imgs1, imgs2)\n",
    "\treturn eu, ss\n",
    "\n",
    "filename = 'results/increase_para/Caltech256.npy'\n",
    "validity = np.load(filename, allow_pickle=True).item()\n",
    "filename_threshold = 'results/SelfOracle/Caltech256_threshold.txt'\n",
    "with open(filename_threshold) as f:\n",
    "\tlines = f.readlines()\n",
    "\tthreshold = float(lines[1].split(':')[1].strip())\n",
    "\n",
    "dataset = 'Caltech256'\n",
    "test_dataset_distance = load_testset(dataset, transform=transform_distance)\n",
    "test_dataload_distance = torch.utils.data.DataLoader(test_dataset_distance, batch_size=batch_size, shuffle=False)\n",
    "source_batches_distance = [batch_source.to(device) for batch_source, _ in test_dataload_distance]\n",
    "pred_source = np.load(f'predictions/{model_name}_source.npy')\n",
    "test_dataset = load_testset(dataset)\n",
    "for index, mr in enumerate(mrs):\n",
    "\tfor para in  paras_more[index]:\n",
    "\t\tdistance_total = []\n",
    "\t\tssim_total = []\n",
    "\t\tfollowup_dataset_distance = CustomDatasetSingleMR(test_dataset, mr=mr, para=para, transform=transform_distance)\n",
    "\t\tfollowup_dataload_distance = torch.utils.data.DataLoader(followup_dataset_distance, batch_size=batch_size, shuffle=False)\n",
    "\t\tfollowup_batches_distance = [batch_source.to(device) for batch_source, _ in followup_dataload_distance]\n",
    "\n",
    "\t\tdataset_followup = CustomDatasetSingleMR(test_dataset, mr=mr, para=para, transform=transform_predict)\n",
    "\t\ttestload_followup = torch.utils.data.DataLoader(dataset_followup, batch_size=batch_size, shuffle=False)\n",
    "\t\tpred_followup = np.zeros(len(pred_source), dtype=int)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i,(X,y) in enumerate(testload_followup):\n",
    "\t\t\t\tX = X.to(device)\n",
    "\t\t\t\tdistances, ssims  = calculate_diff(source_batches_distance[i], followup_batches_distance[i])\n",
    "\t\t\t\tdistance_total.extend(distances)\n",
    "\t\t\t\tssim_total.extend(ssims)\n",
    "\t\t\t\toutputs = model(X)\n",
    "\t\t\t\t_, pred = torch.max(outputs, 1)\n",
    "\t\t\t\tpred_followup[i*batch_size:i*batch_size+X.size(0)] = pred.cpu()\n",
    "\t\taverage_distance, average_ssim = get_average_distance(mrs_name[index], para, distance_total, ssim_total)\n",
    "\t\tfault_num = get_fault_num(mrs_name[index], para, pred_source, pred_followup)\n",
    "\t\tprint(f\"{mrs_name[index]} {para}\\tFault num: {fault_num}\\tEuclidean: {round(average_distance,2)}\\tSSIM: {round(average_ssim,2)}\")\n",
    "\tprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
